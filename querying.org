#+title: Querying


#+BEGIN_SRC python :session querying.org  :exports both
from langchain.vectorstores import Chroma
import indexing

embedding = indexing.EmbeddingConfig.get_default().load_embeddings()
embedding
#+END_SRC

#+RESULTS:
: client=SentenceTransformer(
:   (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel
:   (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
:   (2): Normalize()
: ) model_name='sentence-transformers/all-mpnet-base-v2'

#+BEGIN_SRC python :session querying.org  :exports both
#db = Chroma(persist_directory="vectordb",embedding_function=embedding, collection_name="langchain_rtdocs")
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session querying.org  :exports both
import chromadb

chroma_settings = chromadb.config.Settings(persist_directory="vectordb", chroma_db_impl="duckdb+parquet")
client = chromadb.Client(chroma_settings)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session querying.org  :exports both
vectordb = Chroma(collection_name="text_generation_webui", embedding_function=embedding, client_settings=chroma_settings)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session querying.org  :exports both :results output
for doc in (vectordb.similarity_search("generate from prompt")):
    print(doc.page_content)
    print("#" * 50)
#+END_SRC

#+RESULTS:
#+begin_example
Batches:   0%|                                                                                                        | 0/1 [00:00<?, ?it/s]Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 50.56it/s]
DEBUG:Chroma:time to pre process our knn query: 9.5367431640625e-07
DEBUG:Chroma:time to run knn query: 6.794929504394531e-05
else:

prompt = custom_generate_chat_prompt(text, max_new_tokens, name1, name2, context, chat_prompt_size)

# Yield *Is typing...*

if not regenerate:

yield shared.history['visible']+[[visible_text, shared.processing_message]]

# Generate

reply = ''

for i in range(chat_generation_attempts):
##################################################
max_context = body.get('max_context_length', 2048)

while len(prompt_lines) >= 0 and len(encode('\n'.join(prompt_lines))) > max_context:

prompt_lines.pop(0)

prompt = '\n'.join(prompt_lines)

generator = generate_reply(

question = prompt,

max_new_tokens = body.get('max_length', 200),

do_sample=True,

temperature=body.get('temperature', 0.5),

top_p=body.get('top_p', 1),

typical_p=body.get('typical', 1),

repetition_penalty=body.get('rep_pen', 1.1),

encoder_repetition_penalty=1,
##################################################
result = self()

result.pipeline = pipeline

return result

def generate(self, context="", token_count=20, temperature=1, top_p=1, top_k=50, alpha_frequency=0.1, alpha_presence=0.1, token_ban=[0], token_stop=[], callback=None):

args = PIPELINE_ARGS(

temperature = temperature,

top_p = top_p,

top_k = top_k,

alpha_frequency = alpha_frequency, # Frequency Penalty (as in GPT-3)

alpha_presence = alpha_presence, # Presence Penalty (as in GPT-3)

token_ban = token_ban, # ban the generation of some tokens
##################################################
if 'pygmalion' in shared.model_name.lower():

name1 = "You"

prompt = generate_chat_prompt(text, max_new_tokens, name1, name2, context, chat_prompt_size, impersonate=True)

reply = ''

# Yield *Is typing...*

yield shared.processing_message

for i in range(chat_generation_attempts):
##################################################
#+end_example
